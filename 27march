Q1. R-squared (R²) is a statistical metric used to assess the goodness of fit of a linear regression model. It represents the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features) in the model. In other words, R-squared measures how well the regression line fits the actual data points. The value of R² ranges from 0 to 1, where 0 indicates that the model explains none of the variability in the data, and 1 indicates a perfect fit, where all the variability is explained by the model.

Q2. Adjusted R-squared is a modified version of the regular R-squared, designed to account for the number of independent variables used in the model. It addresses one of the shortcomings of the regular R-squared, which tends to increase with the addition of more independent variables, even if those variables do not contribute meaningfully to the prediction. Adjusted R-squared penalizes the inclusion of irrelevant variables, promoting models with a more parsimonious set of predictors.

The formula for Adjusted R-squared is:
Adjusted R-squared
=
1
−
(
1
−
�
2
)
×
(
�
−
1
)
�
−
�
−
1
Adjusted R-squared=1− 
n−k−1
(1−R 
2
 )×(n−1)
​
 

where 
�
n is the number of data points, and 
�
k is the number of independent variables in the model.

Q3. Adjusted R-squared is more appropriate to use when comparing regression models with different numbers of independent variables. It gives a better indication of the model's generalization ability by adjusting for the potential inclusion of irrelevant predictors. As a result, when choosing among multiple models, the one with the highest adjusted R-squared is generally preferred as it strikes a balance between goodness of fit and model complexity.

Q4. In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are evaluation metrics used to measure the performance of regression models.

RMSE is the square root of the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily.
MSE is the average of the squared differences between predicted and actual values. It is similar to RMSE but without the square root.
MAE is the average of the absolute differences between predicted and actual values. It treats all errors equally and does not penalize large errors as much as RMSE.
The formulas for these metrics are as follows:
RMSE
=
∑
�
=
1
�
(
�
�
−
�
^
�
)
2
�
RMSE= 
n
∑ 
i=1
n
​
 (y 
i
​
 − 
y
^
​
  
i
​
 ) 
2
 
​
 
​
 
MSE
=
∑
�
=
1
�
(
�
�
−
�
^
�
)
2
�
MSE= 
n
∑ 
i=1
n
​
 (y 
i
​
 − 
y
^
​
  
i
​
 ) 
2
 
​
 
MAE
=
∑
�
=
1
�
∣
�
�
−
�
^
�
∣
�
MAE= 
n
∑ 
i=1
n
​
 ∣y 
i
​
 − 
y
^
​
  
i
​
 ∣
​
 

where 
�
n is the number of data points, 
�
�
y 
i
​
  is the actual value of the dependent variable for data point 
�
i, and 
�
^
�
y
^
​
  
i
​
  is the predicted value by the model for data point 
�
i.

Q5. Advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis:

Advantages:

RMSE and MSE give higher weights to large errors, making them suitable for applications where larger errors are more critical.
These metrics are sensitive to outliers, which can be helpful in some cases as outliers may have a significant impact on the model's performance.
They are mathematically convenient for optimization purposes and widely used in various machine learning algorithms.
Disadvantages:

RMSE and MSE are sensitive to the scale of the data and can be influenced by the choice of units used for the dependent variable.
MAE is more robust to outliers, but it can be less sensitive to model improvements due to the use of absolute differences.
All three metrics do not directly provide a measure of the model's goodness of fit or explainability.
Q6. Lasso regularization, also known as L1 regularization, is a technique used to prevent overfitting in linear regression models by adding a penalty term to the standard linear regression cost function. The penalty term is proportional to the absolute values of the regression coefficients. Lasso regularization encourages sparsity in the model, leading to some coefficients being exactly zero, effectively performing feature selection.

The difference between Lasso and Ridge regularization (L2 regularization) lies in the penalty term. Ridge adds a penalty proportional to the squared values of the regression coefficients, which usually results in shrinking the coefficients toward zero but not exactly to zero. Lasso, on the other hand, can lead to exact zero coefficients, effectively removing some features from the model.

When to use Lasso regularization:

When there is a suspicion that some of the features are irrelevant or redundant, and we want a simpler model with fewer predictors.
When feature selection is desired to enhance model interpretability.
When dealing with high-dimensional data, as Lasso can effectively reduce the number of features used.
Q7. Regularized linear models, like Lasso and Ridge regression, help to prevent overfitting in machine learning by imposing penalties on the model's coefficients. Overfitting occurs when a model fits the training data too closely, capturing noise and random variations, which leads to poor performance on unseen data.

By adding regularization, the models are discouraged from assigning excessively high weights to any particular feature. This prevents the model from becoming overly sensitive to the training data and helps it generalize better to new, unseen data. Regularization encourages simpler models with more stable coefficients, which can improve the model's performance on validation or test data.

Example:
Let's consider a simple linear regression problem with two features, x1 and x2. Without regularization, the model might fit the training data perfectly, resulting in complex decision boundaries that capture noise. However, with Lasso or Ridge regularization, the model will constrain the coefficients and simplify the decision boundary, reducing overfitting and improving its performance on unseen data.

Q8. Limitations of regularized linear models:

Feature selection bias: While regularization can help with feature selection by shrinking some coefficients to zero, it may still retain some irrelevant features in the model, especially if their impact is small but nonzero.

Hyperparameter tuning: Regularized models have regularization parameters (alpha/lambda) that need to be set properly. Selecting an appropriate value requires cross-validation or other hyperparameter tuning techniques.

Multicollinearity: Regularization can mitigate the impact of multicollinearity (high correlation between features), but it may not entirely resolve the issue if the collinear features are jointly important predictors.

Nonlinear relationships: Regularized linear models are limited to capturing linear relationships between variables. If the data exhibits complex nonlinear patterns, other algorithms like decision trees or neural networks might be more suitable.

Q9. Choosing between RMSE and MAE depends on the context and the specific problem requirements.

If the goal is to minimize the impact of outliers and large errors, then MAE would be a better choice because it treats all errors equally and is less sensitive to extreme values.
On the other hand, if the focus is on achieving higher accuracy for smaller errors and the presence of outliers is not a major concern, then
